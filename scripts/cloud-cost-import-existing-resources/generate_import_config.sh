#!/bin/bash
#
# Datadog Terraform Import Generator
#
# This script generates Terraform configuration and import commands for existing
# Cloud Cost Management and Tag Pipeline resources in your Datadog account.
#
# PREREQUISITES:
#   - DD_API_KEY: Your Datadog API key (required)
#   - DD_APP_KEY: Your Datadog Application key (required)
#   - DD_SITE: Your Datadog site (optional, defaults to datadoghq.com)
#   - jq: JSON processor tool (required)
#
# USAGE:
#   export DD_API_KEY="your_api_key"
#   export DD_APP_KEY="your_app_key"
#   ./generate_import_config.sh [output_directory]
#
# OUTPUT FILES:
#   - generated_resources.tf: Terraform resource configurations
#   - imports.tf: Terraform import blocks (requires Terraform 1.5+)
#
# SUPPORTED RESOURCES:
#   - AWS CUR (Cost and Usage Report) configurations
#   - Azure Usage Cost configurations
#   - GCP Usage Cost configurations
#   - Custom allocation rules
#   - Tag pipeline rulesets
#
# For more information, see: scripts/README.md

set -e
set -o pipefail

# ============================================================================
# Configuration
# ============================================================================

# Output directory - defaults to current directory if not specified
OUTPUT_DIR="${1:-.}"
GENERATED_TF="${OUTPUT_DIR}/generated_resources.tf"
IMPORT_TF="${OUTPUT_DIR}/imports.tf"

# Datadog API configuration
DD_SITE="${DD_SITE:-datadoghq.com}"
API_BASE="https://api.${DD_SITE}"

# Color codes for terminal output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# ============================================================================
# Prerequisites Check
# ============================================================================
echo -e "${GREEN}Checking prerequisites...${NC}"

# Verify required environment variables are set
if [ -z "$DD_API_KEY" ] || [ -z "$DD_APP_KEY" ]; then
    echo -e "${RED}Error: DD_API_KEY and DD_APP_KEY environment variables must be set${NC}"
    echo ""
    echo "Please export your Datadog credentials:"
    echo "  export DD_API_KEY=\"your_api_key\""
    echo "  export DD_APP_KEY=\"your_app_key\""
    exit 1
fi

# Verify jq is installed for JSON processing
if ! command -v jq &> /dev/null; then
    echo -e "${RED}Error: jq is not installed. Please install it first.${NC}"
    echo ""
    echo "Installation instructions:"
    echo "  macOS: brew install jq"
    echo "  Linux: apt-get install jq / yum install jq"
    exit 1
fi

# Create output directory if it doesn't exist
mkdir -p "$OUTPUT_DIR"

# ============================================================================
# Initialize Output Files
# ============================================================================

# Generate the Terraform configuration file with provider setup
cat > "$GENERATED_TF" <<'EOF'
# ============================================================================
# Generated Terraform Configuration for Datadog Resources
# ============================================================================
#
# This file was auto-generated by generate_import_config.sh
# Generated at: $(date)
#
# IMPORTANT: Review and customize this configuration before applying:
#   - Rename resources to meaningful names
#   - Adjust any configurations to match your requirements
#   - Remove any resources you don't want to manage with Terraform
#
# ============================================================================

terraform {
  required_providers {
    datadog = {
      source = "DataDog/datadog"
    }
  }
}

provider "datadog" {
  api_key = var.datadog_api_key
  app_key = var.datadog_app_key
  api_url = var.datadog_api_url
}

variable "datadog_api_key" {
  description = "Datadog API key"
  type        = string
  sensitive   = true
}

variable "datadog_app_key" {
  description = "Datadog APP key"
  type        = string
  sensitive   = true
}

variable "datadog_api_url" {
  description = "Datadog API URL"
  type        = string
  default     = "https://api.datadoghq.com"
}

EOF

# Generate the Terraform import configuration file
cat > "$IMPORT_TF" <<'EOF'
# ============================================================================
# Terraform Import Configuration for Datadog Resources
# ============================================================================
#
# This file contains import blocks to import existing Datadog resources
# into Terraform state. This uses Terraform's native import blocks feature
# (available in Terraform 1.5+).
#
# Generated at: $(date)
#
# PREREQUISITES:
#   1. Terraform 1.5 or later
#   2. Terraform must be initialized (run: terraform init)
#   3. Datadog credentials must be configured (environment variables or terraform.tfvars)
#
# USAGE:
#   terraform plan -generate-config-out=generated.tf
#   terraform apply
#
#   OR, if you've already customized generated_resources.tf:
#   terraform apply
#
# NOTE: After successful import, you can delete this file or keep it for reference.
#
# ============================================================================

EOF

# ============================================================================
# Helper Functions
# ============================================================================

# api_call: Makes authenticated API calls to Datadog
# Usage: api_call "/api/v2/endpoint"
api_call() {
    local endpoint="$1"
    curl -s -X GET "${API_BASE}${endpoint}" \
        -H "DD-API-KEY: ${DD_API_KEY}" \
        -H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
        -H "Accept: application/json"
}

# ============================================================================
# Fetch Resources from Datadog
# ============================================================================
echo -e "${GREEN}Fetching existing resources from Datadog...${NC}"

# Track total number of resources found
RESOURCE_COUNT=0

# ============================================================================
# AWS Cost and Usage Report (CUR) Configurations
# ============================================================================
echo -e "${YELLOW}Checking for AWS CUR configurations...${NC}"

# Fetch all AWS CUR configurations from Datadog
AWS_CONFIGS=$(api_call "/api/v2/cost/aws_cur_config")
AWS_COUNT=$(echo "$AWS_CONFIGS" | jq '.data | length')

if [ "$AWS_COUNT" -gt 0 ]; then
    echo "  Found $AWS_COUNT AWS CUR configuration(s)"

    # Generate Terraform resource blocks for each AWS CUR config
    echo "$AWS_CONFIGS" | jq -r '.data[] |
        @text "

# AWS CUR Config - Account: \(.attributes.account_id)
resource \"datadog_aws_cur_config\" \"aws_cur_\(.attributes.account_id | gsub("[^a-zA-Z0-9_]"; "_"))\" {
  account_id    = \"\(.attributes.account_id)\"
  bucket_name   = \"\(.attributes.bucket_name)\"
  bucket_region = \"\(.attributes.bucket_region // "")\"
  report_name   = \"\(.attributes.report_name)\"
  report_prefix = \"\(.attributes.report_prefix)\"
\(if .attributes.account_filters and (.attributes.account_filters.include_new_accounts != null or (.attributes.account_filters.excluded_accounts | length) > 0 or (.attributes.account_filters.included_accounts | length) > 0) then "
  account_filters {
    include_new_accounts = \(.attributes.account_filters.include_new_accounts // false)
\(if .attributes.account_filters.excluded_accounts and (.attributes.account_filters.excluded_accounts | length > 0) then "    excluded_accounts    = [\"\(.attributes.account_filters.excluded_accounts | join("\", \""))\"]" else "" end)
\(if .attributes.account_filters.included_accounts and (.attributes.account_filters.included_accounts | length > 0) then "    included_accounts    = [\"\(.attributes.account_filters.included_accounts | join("\", \""))\"]" else "" end)
  }" else "" end)
}
"' >> "$GENERATED_TF"

    # Generate import blocks for each AWS CUR config
    echo "$AWS_CONFIGS" | jq -r '.data[] |
        @text "
import {
  to = datadog_aws_cur_config.aws_cur_\(.attributes.account_id | gsub("[^a-zA-Z0-9_]"; "_"))
  id = \"\(.id)\"
}
"' >> "$IMPORT_TF"

    RESOURCE_COUNT=$((RESOURCE_COUNT + AWS_COUNT))
fi

# ============================================================================
# Azure Usage Cost (UC) Configurations
# ============================================================================
echo -e "${YELLOW}Checking for Azure UC configurations...${NC}"

# Fetch list of Azure UC config IDs
# Note: Azure API returns a list, then we fetch details for each config
AZURE_RESPONSE=$(api_call "/api/v2/cost/azure_uc_config")
AZURE_IDS=$(echo "$AZURE_RESPONSE" | jq -r '.data[].id' 2>/dev/null || echo "")

if [ -n "$AZURE_IDS" ]; then
    AZURE_COUNT=$(echo "$AZURE_IDS" | wc -l | tr -d ' ')
    echo "  Found $AZURE_COUNT Azure UC configuration(s)"

    # Fetch and process each Azure UC config individually
    for CLOUD_ACCOUNT_ID in $AZURE_IDS; do
        AZURE_CONFIG=$(api_call "/api/v2/cost/azure_uc_config/${CLOUD_ACCOUNT_ID}")

        # Generate Terraform resource block for this Azure config
        # Note: Azure configs support two dataset types: "actual" and "amortized"
        echo "$AZURE_CONFIG" | jq -r '
            .data.attributes.configs as $configs |
            ($configs | map(select(.dataset_type == "actual")) | first) as $actual |
            ($configs | map(select(.dataset_type == "amortized")) | first) as $amortized |
            ($configs | first) as $first |
            @text "

# Azure UC Config - Account: \($first.account_id)
resource \"datadog_azure_uc_config\" \"azure_uc_\($first.account_id | gsub("[^a-zA-Z0-9_]"; "_"))\" {
  account_id = \"\($first.account_id)\"
  client_id  = \"\($first.client_id)\"
  scope      = \"\($first.scope)\"

\(if $actual then "  actual_bill_config {
    export_name       = \"\($actual.export_name)\"
    export_path       = \"\($actual.export_path)\"
    storage_account   = \"\($actual.storage_account)\"
    storage_container = \"\($actual.storage_container)\"
  }
" else "" end)
\(if $amortized then "  amortized_bill_config {
    export_name       = \"\($amortized.export_name)\"
    export_path       = \"\($amortized.export_path)\"
    storage_account   = \"\($amortized.storage_account)\"
    storage_container = \"\($amortized.storage_container)\"
  }
" else "" end)}
"' >> "$GENERATED_TF"

        # Generate import block for this Azure config
        AZURE_ACCOUNT_ID=$(echo "$AZURE_CONFIG" | jq -r '.data.attributes.configs[0].account_id | gsub("[^a-zA-Z0-9_]"; "_")')
        cat >> "$IMPORT_TF" <<AZUREIMPORT

import {
  to = datadog_azure_uc_config.azure_uc_${AZURE_ACCOUNT_ID}
  id = "${CLOUD_ACCOUNT_ID}"
}
AZUREIMPORT
        RESOURCE_COUNT=$((RESOURCE_COUNT + 1))
    done
fi

# ============================================================================
# GCP Usage Cost (UC) Configurations
# ============================================================================
echo -e "${YELLOW}Checking for GCP UC configurations...${NC}"

# Fetch list of GCP UC config IDs
GCP_RESPONSE=$(api_call "/api/v2/cost/gcp_uc_config")
GCP_IDS=$(echo "$GCP_RESPONSE" | jq -r '.data[].id' 2>/dev/null || echo "")

if [ -n "$GCP_IDS" ]; then
    GCP_COUNT=$(echo "$GCP_IDS" | wc -l | tr -d ' ')
    echo "  Found $GCP_COUNT GCP UC configuration(s)"

    # Fetch and process each GCP UC config individually
    for CLOUD_ACCOUNT_ID in $GCP_IDS; do
        GCP_CONFIG=$(api_call "/api/v2/cost/gcp_uc_config/${CLOUD_ACCOUNT_ID}")

        # Generate Terraform resource block for this GCP config
        echo "$GCP_CONFIG" | jq -r '.data |
            @text "

# GCP UC Config - Billing Account: \(.attributes.account_id)
resource \"datadog_gcp_uc_config\" \"gcp_uc_\(.attributes.account_id | gsub("[^a-zA-Z0-9_]"; "_"))\" {
  billing_account_id  = \"\(.attributes.account_id)\"
  bucket_name         = \"\(.attributes.bucket_name)\"
  export_dataset_name = \"\(.attributes.dataset)\"
\(if .attributes.export_prefix and .attributes.export_prefix != "" then "  export_prefix       = \"\(.attributes.export_prefix)\"" else "" end)
  export_project_name = \"\(.attributes.export_project_name)\"
  service_account     = \"\(.attributes.service_account)\"
}
"' >> "$GENERATED_TF"

        # Generate import block for this GCP config
        GCP_BILLING_ACCOUNT=$(echo "$GCP_CONFIG" | jq -r '.data.attributes.account_id | gsub("[^a-zA-Z0-9_]"; "_")')
        cat >> "$IMPORT_TF" <<GCPIMPORT

import {
  to = datadog_gcp_uc_config.gcp_uc_${GCP_BILLING_ACCOUNT}
  id = "${CLOUD_ACCOUNT_ID}"
}
GCPIMPORT
        RESOURCE_COUNT=$((RESOURCE_COUNT + 1))
    done
fi

# ============================================================================
# Custom Allocation Rules
# ============================================================================
echo -e "${YELLOW}Checking for custom allocation rules...${NC}"

# Fetch all custom allocation rules
# Note: These rules allow custom distribution of cloud costs
ALLOCATION_RESPONSE=$(api_call "/api/v2/cost/arbitrary_rule" 2>/dev/null || echo '{"data":[]}')
ALLOCATION_COUNT=$(echo "$ALLOCATION_RESPONSE" | jq '.data | length' 2>/dev/null || echo "0")

if [ "$ALLOCATION_COUNT" -gt 0 ]; then
    echo "  Found $ALLOCATION_COUNT custom allocation rule(s)"

    # Process each allocation rule
    echo "$ALLOCATION_RESPONSE" | jq -c '.data[]' | while read -r rule; do
        # Extract rule metadata
        RULE_ID=$(echo "$rule" | jq -r '.id')
        RULE_NAME=$(echo "$rule" | jq -r '.attributes.rule_name')
        RULE_NAME_SAFE=$(echo "$RULE_NAME" | sed 's/[^a-zA-Z0-9_]/_/g' | tr '[:upper:]' '[:lower:]')
        STRATEGY_TYPE=$(echo "$rule" | jq -r '.attributes.strategy.strategy_type // "proportional"')

        # Generate the Terraform resource block header and common fields
        cat >> "$GENERATED_TF" <<RULEEOF

# Custom Allocation Rule - ${RULE_NAME}
resource "datadog_custom_allocation_rule" "custom_allocation_rule_${RULE_NAME_SAFE}" {
  rule_name     = $(echo "$rule" | jq -r '.attributes.rule_name | @json')
  enabled       = $(echo "$rule" | jq -r '.attributes.enabled')
  providernames = $(echo "$rule" | jq -r '.attributes.provider | @json')

$(echo "$rule" | jq -r '.attributes.costs_to_allocate[]? |
  select(.condition != null and .tag != null) |
  "  costs_to_allocate {
    condition = \"\(.condition)\"
    tag       = \"\(.tag)\"" +
    (if (.values | type) == "array" and (.values | length > 0) then "\n    values    = " + (.values | @json) elif (.value | type) == "string" and .value != "" then "\n    value     = \"\(.value)\"" else "" end) +
  "\n  }"')

RULEEOF

        # Generate the strategy block
        # All strategies use a single "strategy" block with a "method" field
        # Method can be: even, proportional, proportional_timeseries, or percent
        echo "$rule" | jq -r '.attributes.strategy |
  "  strategy {" +
  (if .granularity then "\n    granularity = \"\(.granularity)\"" else "" end) +
  (if .method then "\n    method      = \"\(.method)\"" else "" end) +
  (if .allocated_by_tag_keys then "\n    allocated_by_tag_keys = " + (.allocated_by_tag_keys | @json) else "" end) +
  (if .evaluate_grouped_by_tag_keys then "\n    evaluate_grouped_by_tag_keys = " + (.evaluate_grouped_by_tag_keys | @json) else "" end) +
  (if .based_on_costs then
    (.based_on_costs |
      map(select(.condition != null and .tag != null)) |
      map("\n    based_on_costs {
      condition = \"\(.condition)\"
      tag       = \"\(.tag)\"" +
      (if (.values | type) == "array" and (.values | length > 0) then "\n      values    = " + (.values | @json) elif (.value | type) == "string" and .value != "" then "\n      value     = \"\(.value)\"" else "" end) +
    "\n    }") | join(""))
  else "" end) +
  (if .allocated_by_filters then
    (.allocated_by_filters |
      map(select(.condition != null and .tag != null)) |
      map("\n    allocated_by_filters {
      condition = \"\(.condition)\"
      tag       = \"\(.tag)\"" +
      (if (.values | type) == "array" and (.values | length > 0) then "\n      values    = " + (.values | @json) elif (.value | type) == "string" and .value != "" then "\n      value     = \"\(.value)\"" else "" end) +
    "\n    }") | join(""))
  else "" end) +
  (if .evaluate_grouped_by_filters then
    (.evaluate_grouped_by_filters |
      map(select(.condition != null and .tag != null)) |
      map("\n    evaluate_grouped_by_filters {
      condition = \"\(.condition)\"
      tag       = \"\(.tag)\"" +
      (if (.values | type) == "array" and (.values | length > 0) then "\n      values    = " + (.values | @json) elif (.value | type) == "string" and .value != "" then "\n      value     = \"\(.value)\"" else "" end) +
    "\n    }") | join(""))
  else "" end) +
  (if .based_on_timeseries then "\n    based_on_timeseries {}\n" else "" end) +
  (if .allocated_by then
    (.allocated_by |
      # First filter: only keep allocated_by blocks that have valid allocated_tags
      map(select(
        .percentage != null and
        .allocated_tags != null and
        (.allocated_tags | length > 0) and
        ([.allocated_tags[] | select(.key != null and .value != null)] | length > 0)
      )) |
      # Then generate the blocks
      map("\n    allocated_by {
      percentage = \(.percentage)" +
        (.allocated_tags | map(select(.key != null and .value != null)) | map("\n      allocated_tags {
        key   = \"\(.key)\"
        value = \"\(.value)\"
      }") | join("")) +
      "\n    }") | join(""))
  else "" end) +
  "\n  }"' >> "$GENERATED_TF"

        # Close the resource block
        echo "}" >> "$GENERATED_TF"

        # Generate import block for this rule
        cat >> "$IMPORT_TF" <<RULEIMPORT

import {
  to = datadog_custom_allocation_rule.custom_allocation_rule_${RULE_NAME_SAFE}
  id = "${RULE_ID}"
}
RULEIMPORT
    done

    RESOURCE_COUNT=$((RESOURCE_COUNT + ALLOCATION_COUNT))
else
    echo "  No custom allocation rules found"
fi

# ============================================================================
# Tag Pipeline Rulesets
# ============================================================================
echo -e "${YELLOW}Checking for tag pipeline rulesets...${NC}"

# Fetch all tag pipeline rulesets
# Note: Tag pipelines enrich metrics and logs with additional tags based on rules
TAG_PIPELINE_RESPONSE=$(api_call "/api/v2/tags/enrichment" 2>/dev/null || echo '{"data":[]}')
TAG_PIPELINE_COUNT=$(echo "$TAG_PIPELINE_RESPONSE" | jq '.data | length' 2>/dev/null || echo "0")

if [ "$TAG_PIPELINE_COUNT" -gt 0 ]; then
    echo "  Found $TAG_PIPELINE_COUNT tag pipeline ruleset(s)"

    # Process each tag pipeline ruleset
    echo "$TAG_PIPELINE_RESPONSE" | jq -c '.data[]' | while read -r ruleset; do
        # Extract ruleset metadata
        RULESET_ID=$(echo "$ruleset" | jq -r '.id')
        RULESET_NAME=$(echo "$ruleset" | jq -r '.attributes.name')
        RULESET_NAME_SAFE=$(echo "$RULESET_NAME" | sed 's/[^a-zA-Z0-9_]/_/g' | tr '[:upper:]' '[:lower:]')
        RULESET_ID_SAFE=$(echo "$RULESET_ID" | sed 's/[^a-zA-Z0-9_]/_/g')

        # Generate the Terraform resource block for this tag pipeline
        echo "" >> "$GENERATED_TF"
        echo "# Tag Pipeline Ruleset - ${RULESET_NAME} (ID: ${RULESET_ID})" >> "$GENERATED_TF"
        echo "resource \"datadog_tag_pipeline_ruleset\" \"tag_pipeline_ruleset_${RULESET_NAME_SAFE}_${RULESET_ID_SAFE}\" {" >> "$GENERATED_TF"
        echo "  name    = $(echo "$ruleset" | jq -r '.attributes.name | @json')" >> "$GENERATED_TF"
        echo "  enabled = $(echo "$ruleset" | jq -r '.attributes.enabled')" >> "$GENERATED_TF"

        # Check if there are any rules
        RULES_COUNT=$(echo "$ruleset" | jq -r '.attributes.rules | length')

        if [ "$RULES_COUNT" -gt 0 ]; then
            echo "" >> "$GENERATED_TF"

            # Generate rules using jq - all in one go to avoid subshell issues
            echo "$ruleset" | jq -r '.attributes.rules[] |
  "  rules {" +
  "\n    name    = " + (.name | @json) +
  "\n    enabled = " + (.enabled | tostring) +
  "\n" +
  (if .mapping then
    "\n    mapping {" +
    "\n      destination_key = \"" + .mapping.destination_key + "\"" +
    "\n      if_not_exists   = " + (.mapping.if_not_exists | tostring) +
    "\n      source_keys     = " + (.mapping.source_keys | @json) +
    "\n    }"
  elif .query then
    "\n    query {" +
    "\n      query         = " + (.query.query | @json) +
    "\n      if_not_exists = " + (.query.if_not_exists | tostring) +
    (if .query.case_insensitivity then "\n      case_insensitivity = " + (.query.case_insensitivity | tostring) else "" end) +
    "\n" +
    (if .query.addition then
      "\n      addition {" +
      "\n        key   = " + (.query.addition.key | @json) +
      "\n        value = " + (.query.addition.value | @json) +
      "\n      }"
    else "" end) +
    "\n    }"
  elif .reference_table then
    "\n    reference_table {" +
    "\n      table_name         = \"" + .reference_table.table_name + "\"" +
    "\n      if_not_exists      = " + (.reference_table.if_not_exists | tostring) +
    "\n      source_keys        = " + (.reference_table.source_keys | @json) +
    (if .reference_table.case_insensitivity then "\n      case_insensitivity = " + (.reference_table.case_insensitivity | tostring) else "" end) +
    "\n" +
    (.reference_table.field_pairs | map(
      "\n      field_pairs {" +
      "\n        input_column = \"" + .input_column + "\"" +
      "\n        output_key   = \"" + .output_key + "\"" +
      "\n      }"
    ) | join("")) +
    "\n    }"
  else "" end) +
  "\n  }"' >> "$GENERATED_TF"

            echo "" >> "$GENERATED_TF"
        fi

        echo "}" >> "$GENERATED_TF"

        # Generate import block for this ruleset
        cat >> "$IMPORT_TF" <<TAGIMPORT

import {
  to = datadog_tag_pipeline_ruleset.tag_pipeline_ruleset_${RULESET_NAME_SAFE}_${RULESET_ID_SAFE}
  id = "${RULESET_ID}"
}
TAGIMPORT
    done

    RESOURCE_COUNT=$((RESOURCE_COUNT + TAG_PIPELINE_COUNT))
else
    echo "  No tag pipeline rulesets found"
fi

# ============================================================================
# Finalize and Display Summary
# ============================================================================

# Display summary
echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}Generation complete!${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""
echo "Total resources found: $RESOURCE_COUNT"
echo ""
echo "Generated files:"
echo "  - ${GENERATED_TF}"
echo "  - ${IMPORT_TF}"
echo ""

# Display next steps only if resources were found
if [ "$RESOURCE_COUNT" -gt 0 ]; then
    echo "Next steps:"
    echo ""
    echo "  1. Review and customize ${GENERATED_TF}:"
    echo "     - Rename resources to meaningful names"
    echo "     - Adjust configurations as needed"
    echo "     - Remove any resources you don't want to manage"
    echo ""
    echo "  2. Ensure Terraform provider is configured with credentials:"
    echo "     Option A: Environment variables (recommended):"
    echo "       export DD_API_KEY=\"your_api_key\""
    echo "       export DD_APP_KEY=\"your_app_key\""
    echo ""
    echo "     Option B: Or create a terraform.tfvars file:"
    echo "       datadog_api_key = \"your_api_key\""
    echo "       datadog_app_key = \"your_app_key\""
    echo ""
    echo "  3. Initialize Terraform (if not already done):"
    echo "     terraform init"
    echo ""
    echo "  4. Import all resources into Terraform state:"
    echo "     terraform apply"
    echo ""
    echo "     This will process all import blocks in ${IMPORT_TF}"
    echo "     and import the resources into your Terraform state."
    echo ""
    echo "  5. Verify the import succeeded:"
    echo "     terraform plan"
    echo ""
    echo "     You should see 'No changes' if everything matches correctly."
    echo ""
    echo "  6. (Optional) Clean up the imports file:"
    echo "     After successful import, you can delete ${IMPORT_TF}"
    echo "     or keep it for reference."
    echo ""
    echo "NOTE: This script uses Terraform's native import blocks (Terraform 1.5+)."
    echo "      If you're using an older version of Terraform, please upgrade or use"
    echo "      the legacy 'terraform import' commands instead."
    echo ""
else
    # No resources found - provide troubleshooting guidance
    echo -e "${YELLOW}========================================${NC}"
    echo -e "${YELLOW}Warning: No resources were found${NC}"
    echo -e "${YELLOW}========================================${NC}"
    echo ""
    echo "This could mean:"
    echo "  - You have no existing Cloud Cost Management or Tag Pipeline resources in Datadog"
    echo "  - Your API keys don't have sufficient permissions to read these resources"
    echo "  - The API endpoints are not available for your Datadog site (${DD_SITE})"
    echo ""
    echo "Troubleshooting:"
    echo "  1. Verify your API keys have the correct permissions:"
    echo "     - Cloud Cost Management (read)"
    echo "     - Tag Pipelines (read)"
    echo ""
    echo "  2. Check if you're using the correct Datadog site:"
    echo "     Current: ${DD_SITE}"
    echo "     To change: export DD_SITE=\"your-site.datadoghq.com\""
    echo ""
    echo "  3. Verify you have resources in your Datadog account:"
    echo "     - Log into Datadog UI and check Cloud Cost Management"
    echo "     - Check for any Tag Pipeline configurations"
    echo ""
fi
